# Sears Home Services Technician Analytics Platform - Progress Log

## Codebase Patterns

READ THIS SECTION FIRST - These patterns have been established for this codebase:

### Project Structure
- All application code lives in `app/` directory
- Services go in `app/services/` - one service per concern
- ML services go in `app/services/ml/` - InterpretML, Prophet
- Routers go in `app/routers/` - one router per domain
- Models go in `app/models/` - SQLAlchemy models with `__init__.py` exports
- Schemas go in `app/schemas/` - Pydantic models for API
- Static files in `app/static/` - dashboard HTML/CSS/JS
- Trained models stored in `/models/` volume mount

### Async Patterns
- ALWAYS use async/await - never blocking calls
- Use `asyncpg` for PostgreSQL, not `psycopg2`
- Use `redis.asyncio` not sync `redis`
- Use `aiohttp` or async OpenAI client for HTTP calls
- Snowflake: use `run_in_executor` for sync connector calls
- Session management: use `async with` for database sessions

### Database Patterns
- PGVector: Use HNSW index with `vector_cosine_ops` for similarity
- Embedding dimension: 1536 (OpenAI text-embedding-3-small)
- Always use UUID primary keys: `Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)`
- Timestamps: `created_at` and `updated_at` on all models
- Use JSONB for flexible metadata fields (metrics, explanations)

### Snowflake Patterns
- Key-pair JWT authentication with private key file
- Connection: use snowflake-connector-python
- Queries: wrap in run_in_executor for async compatibility
- Cache results in Redis with configurable TTL
- Tables in PRD_AGENTIC_SOLUTIONS.DATA_SOURCES schema

### ML Model Patterns
- InterpretML for interpretable models (EBM, Decision Tree)
- Prophet for time-series forecasting
- Serialize models with joblib to .sav files
- Store in /models/{model_type}/{version}/ directory
- Track model metadata in TrainedModel table
- Explanations: global (feature importance) + local (per-prediction)

### Dashboard Patterns
- Simple HTML/JS served by FastAPI static files
- Chart.js for visualizations (line charts, bar charts)
- Prophet forecast: line chart with confidence intervals
- Feature importance: horizontal bar chart
- Real-time updates via SSE for long-running operations

### Agent Patterns
- Domain-specific system prompt for technician analytics
- Tools for querying data, predictions, forecasts, ROI calculations
- Natural language explanations: "Your X adds Y to the score..."
- Streaming responses via SSE
- Session state in Redis

### FastAPI Patterns
- Use lifespan context manager for startup/shutdown
- Dependency injection for services via `Depends()`
- Background tasks for model training (non-blocking)
- Always include response_model in route decorators

### SSE Patterns
- Use `StreamingResponse` with `media_type="text/event-stream"`
- Format: `data: {json}\n\n`
- Always send `data: [DONE]\n\n` at stream end
- Disable buffering in Nginx for SSE routes

### Testing Patterns
- Use `pytest-asyncio` for async tests
- Fixtures in `conftest.py` for shared setup
- Mock external APIs (OpenAI, Snowflake) in unit tests
- Integration tests use real Docker services

### Naming Conventions
- Services: `{Domain}Service` (e.g., `InterpretMLService`, `SnowflakeService`)
- Routers: lowercase domain (e.g., `dashboard.py`, `agent.py`)
- Models: PascalCase singular (e.g., `Technician`, `Prediction`)
- Schemas: `{Model}Create`, `{Model}Response`, `{Model}Update`

---

## Story Progress

### US-006: InterpretML Training Service - COMPLETE
- Created `app/services/ml/interpret_service.py` with `InterpretMLService` class
- EBM training: `train_ebm()` with ExplainableBoostingClassifier
- Decision Tree training: `train_decision_tree()` with ClassificationTree
- Global explanations: `get_global_explanations()` - feature importance from explain_global()
- Local explanations: `get_local_explanation()` - per-prediction with explain_local()
- Natural language generation: `_generate_natural_language_explanation()` - human-readable risk explanations
- Batch predictions: `predict_batch()` with risk classification (LOW/MEDIUM/HIGH)
- Model serialization to joblib .sav files in /models/{model_type}/{version}/
- 5 tests passing in `tests/test_interpret_service.py`

**Learnings:**
- InterpretML ClassificationTree has different explain_global() data structure than EBM
- Use hasattr() fallback for feature_importances_ if names/scores not in exp_data
- Need `# type: ignore[arg-type]` for sklearn's zero_division parameter
- Python 3.9 needs `Optional[]` instead of `|` union syntax for SQLAlchemy type hints
- Add ruff.toml per-file ignores for UP045 (Optional instead of |) in models and ML services

### US-007: Prophet Forecasting Service - COMPLETE
- Created `app/services/ml/prophet_service.py` with `ProphetService` class
- Train forecast: `train_forecast()` with configurable seasonality
- Future predictions: `predict_future(periods)` with confidence intervals
- Components: `get_trend_components()` - trend, yearly, weekly, daily seasonality
- Convenience methods: `forecast_technician_headcount()`, `forecast_job_demand()`
- Chart-friendly output: dates, yhat, yhat_lower, yhat_upper
- Metrics: MAE, MAPE, RMSE for forecast accuracy
- Model serialization to joblib .sav in /models/prophet/{forecast_type}/{version}.sav
- 7 tests passing in `tests/test_prophet_service.py`

**Learnings:**
- Prophet has poor type hints - need `# type: ignore[arg-type]` for seasonality params
- Use numpy arrays instead of pandas Series for metric calculations (better pyright support)
- Prophet warnings about divide by zero in matmul are normal (internal calculations)
- Store model parameter `model` in `_extract_components` kept for API consistency even if unused

### US-008: Model Training API Endpoints - COMPLETE
- Created `app/routers/models.py` with model management endpoints
- Created `app/schemas/models.py` with Pydantic request/response schemas
- Training endpoints: POST /train/ebm, /train/decision-tree, /train/prophet
- Training runs as FastAPI background tasks
- Training status tracked in Redis with 1hr TTL
- GET /training/{job_id}/status to check progress
- List endpoints: GET /api/v1/models with filtering by type, is_active
- GET /{id}/explanations for global feature importance
- POST /{id}/predict for predictions with local explanations
- GET /{id}/forecast for Prophet future predictions
- DELETE /{id} for soft delete (sets is_active=False)

**Learnings:**
- Python 3.9 requires `Optional[X]` for FastAPI query params, not `X | None`
- Add `UP045` to ruff ignores for routers (Python 3.9 compatibility)
- FastAPI evaluates type annotations at runtime, so need actual imports not TYPE_CHECKING
- Background tasks capture dependencies from outer scope (closure)
- Database is `get_db` not `get_session` in this codebase

### US-009: Embedding Service - COMPLETE
- Created `app/services/embeddings.py` with `EmbeddingService` class
- OpenAI text-embedding-3-small (1536 dimensions)
- Batch embedding support for bulk operations
- Technician profile text generation from structured data
- Cosine similarity search implementation
- 10 tests passing

### US-010: Dashboard API Endpoints - COMPLETE
- Created `app/routers/dashboard.py` with dashboard endpoints
- GET /overview for metrics cards
- GET /forecast for Prophet chart data
- GET /feature-importance for model explanations
- GET /regions for regional summary
- GET /technicians for paginated list with predictions
- Created `app/schemas/dashboard.py` with Chart.js optimized schemas

### US-011: Dashboard Frontend - COMPLETE
- Created responsive HTML/CSS/JS dashboard in `app/static/index.html`
- Chart.js for line and bar charts
- Placeholder charts when no trained models exist
- Regional summary table with risk badges
- Technicians at risk list
- Chat widget placeholder

### US-012: Tool Definition Framework - COMPLETE
- Created `app/services/tools.py` with ToolRegistry class
- ToolDefinition pydantic model with JSON Schema parameters
- Async tool execution with error handling
- Schema export for Claude and OpenAI function calling formats
- @tool decorator for marking functions
- 16 tests in `tests/test_tools.py`

### US-013: Agent Tools - COMPLETE
- Created `app/services/agent_tools.py` with 6 technician analytics tools:
  - query_technicians: search by region, status, risk
  - get_prediction: retention prediction with explanation
  - get_forecast: Prophet forecast with confidence intervals
  - calculate_roi: ROI for retention interventions
  - get_feature_importance: ranked features from ML models
  - get_regional_summary: metrics by region
- Natural language explanation generation
- 20 tests in `tests/test_agent_tools.py`

### US-014: Agent Loop Implementation - COMPLETE
- Created `app/services/agent.py` with AgentService class
- AgentState dataclass for conversation management
- AgentEvent dataclass with SSE formatting
- Integration with Claude API (anthropic library)
- Streaming token-by-token responses
- Tool execution loop with max iterations
- Domain-specific system prompt for technician analytics
- 15 tests with mocked Claude responses

### US-015: Agent Chat Endpoints - COMPLETE
- Created `app/routers/agent.py` with endpoints:
  - POST /api/v1/agent/chat - synchronous chat
  - GET /api/v1/agent/stream - SSE streaming
  - GET/DELETE /sessions/{id} - session management
  - GET /examples - suggested prompts
- Redis session storage with 1hr TTL
- Example prompts for common queries
- 18 tests in `tests/test_agent_router.py`

### US-016: Agent Chat UI - COMPLETE
- Updated `app/static/index.html` with functional chat widget
- EventSource (SSE) connection to agent stream
- Real-time token streaming with typing indicator
- Tool call visualization with status icons
- Session ID management for conversation continuity
- Clear chat button, markdown formatting
- Dynamic example prompts from API

---

## Blockers and Notes

- Snowflake private key file (sf_rsa_key.p8) must be provided separately - NOT in git
- Need to discover actual table schemas in Snowflake DATA_SOURCES schema

---

## Session Log

Started: 2024-01-06
Project: Sears Home Services Technician Analytics Platform
Goal: Predict technician recruitment/retention with explainable AI
